\section{Discussion}
Looking at the results we can observe that team Q-learning converged too a solution faster than single Q-learning. This was slightly the case for the first stage of the simulation (getting to the block), but quit considerably so for the second stage (getting the block the goal). On the other hand, the single Q-learning solution turned out more optimal than its counterpart, with on average 6.5 less steps than the team Q-learning solution. These are however, the overall results of the simulation. It is easier and more insightful to discuss the more specific phenomena we encountered in the results.

First, let us look into the two stages of the simulation separately. Before the block has been grasped we see little difference between both of the algorithms in the average number of epochs it takes before convergence (2609 vs. 2512), but we do see a significant difference in the average number of steps taken in the final solution (31.9 vs. 38.1). This difference is quite high: 6.5 steps on a total of $\pm 35.15$ steps comes down to approximately $18.5\%$. If the state space would be of a higher order, this could grow to become a considerable drawback to using team-Q learning, since its solutions would be much less optimal. The fact that the single Q learning algorithm resulted in overall better solutions is rather curious, especially given the environment we used for the simulation. 
To get to the block, the agents first have to get through a one cell wide gap. We expected that if the agents did not cooperate, they would both try to get through the gap simultaneously, which would result in a hold-up. Team Q-learning would then ensure that the agents would go through the gap in a logical order. Therefore it is hard to explain these results.
It might be the case that our results are skewed because of the fact that we used just one environment. In future research, it might be better to run the simulation on multiple environments and average the results, to get more representative results.

In the second stage of the simulation, we see a different phenomenon. Instead of a difference in solution (this turned out insignificant), we see a difference in convergence speed, and quite a considerable one. Where it took single Q-learning on average 1772 epochs, team Q-learning finished in 290 epochs. This comes down to a factor $\pm 6.1$ faster convergence. This is most probably due to the fact that after the agents have grasped the block, they can only move it if they perform the same action. This requires some coordination, and that is exactly where team Q-learning's strength lies. With team Q-learning, the algorithm learns that moving in different directions never accomplishes anything, while it is not possible to figure this out with single-agent Q-learning.

Secondly, the progression of the standard deviations (figures \ref{3a} and \ref{3b}) show interesting phenomena as well. In the non-grasped condition, the standard deviation is a little higher for team Q-learning, which might indicate that team Q-learning tries out more different solutions to the problem. In the grasped condition, the standard deviation for team Q-learning is a lot lower in later epochs, most likely because the algorithm just converges to one solution faster.

Finally, let us look at the curves of the epochs themselves (figure \ref{fig:Results1}). For single Q-learning, there is a small dip at around 250 epochs for both the grasped and non-grasped conditions. We checked for this phenomenon in more runs and this happens to be the case most of the times. The cause of this dip might have to do with the fact that there is more than one agent in the simulation. The Q-value of a state is not only dependent on the actions of an agents himself, but on those of the other agent as well. So initially an agents might have found a good path, but as soon as one of the agents prefers an action that prevents the other agent from choosing its best path (which is quite thinkable in our environment), the latter agent has to learn a new path for the new situation. 

In conclusion, both of the algorithms seem to have their pros and cons. Team Q-learning has a better convergence speed, but finds less optimal solutions. A combination of these two algorithms might therefore be the most optimal: let single Q-learning find the best path to grasp the block, and let team Q-learning find the best way to bring the block to the goal. However, to truly measure the performance of both algorithms, testing should be done using different environments. Other interesting areas of further research might involve extending the simulation with more than two agents, multiple goal areas or different blocks.