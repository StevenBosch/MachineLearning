\section{Introduction}
One of the returning issues in machine learning is that it is often hard to decide which learning algorithm should be used for a certain application. Over the years, a lot of valid learning algorithms have emerged, each with their own advantages and disadvantages. Some algorithms might be faster, while others might be more precise, and an algorithm that is better in one area usually performs worse in another. Matching the right task with the right algorithm is not always a straightforward exercise.

In this paper we will look at the problem of multi-agent box-transportation.  For this task, two virtual robots will have to cooperate to transport a box to a certain goal area, while avoiding certain obstacles. This is a problem that has already gathered some attention in machine learning research. In 2006, Wang and de Silva tried to solve this problem with reinforcement learning techniques \cite{wang2006}. They compared the performance of a Q-learning algorithm with the performance of team Q-learning in a dynamic environment. In 2010, a team at the University of Delft also showed how you could use Q-learning, team Q-learning and the WoLF-PHC algorithm to accomplish a similar task in a static environment \cite{busoniu2010}.

In line with the mentioned studies we will compare the Q-learning and team Q-learning algorithm, since they seem most suitable for a task of this sort. To do so, we have created a simulation in which two agents have to transport a box in a two dimensional static environment. The comparison in performance of our two reinforcement learning algorithms will be made by looking at the time it takes for them to converge to a solution and the optimality of the found solution. 