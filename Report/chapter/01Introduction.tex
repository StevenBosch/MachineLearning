\section{Introduction}
One of the returning issues in machine learning is that it is often hard to decide which learning algorithm should be used for a certain application. Over the years, a lot of valid learning algorithms have emerged, each with their own advantages and disadvantages. Some algorithms might be faster, while others might be more precise, and an algorithm that is better in one area usually performs worse in another. Matching the right task with the right algorithm is not always a straightforward exercise.

In this paper we will look at the problem of multi-agent box-transportation.  For this task, two virtual robots will have to cooperate to transport a box to a certain goal area, while avoiding certain obstacles. This is a problem that has already garnered some attention in machine learning research. In 2006, Wang and de Silva tried to solve this problem with reinforcement learning techniques \cite{wang2006}. They compared the performance of a Q-learning algorithm with the performance of team Q-learning in a dynamic environment. In 2010, a team at the University of Deft also showed how you could use Q-learning, team Q-learning and the WoLF-PHC algorithm to accomplish a similar task in a static environment \cite{busoniu2010}.

We will also compare the Q-learning and team Q-learning algorithm, since they seem most suitable for a task like this. For our initial task, two agents have to transport a box in a two dimensional static environment. We will compare the performance of our two reinforcement learning algorithms by looking at the time it takes for them to converge to the optimal solution (given that they find it). We have created a simulation to do so, which will provide us with the results we need.
